{
 "metadata": {
  "name": "",
  "signature": "sha256:64c7fabfca4378727f1709f741769ec71b47da4316846c6e92b987d6f0a736e1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# If this gives error, go to command line and run \"conda install nltk\"\n",
      "import nltk"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# NLTK comes with its own set of corpus data that we can use to learn about NLTK\n",
      "nltk.download()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "showing info http://nltk.github.com/nltk_data/\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Corpus\n",
      "\n",
      "* \"corpus\" = collection of documents\n",
      "* \"corpora\" = plural form of corpus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import inaugural"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "inaugural.fileids()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "[u'1789-Washington.txt',\n",
        " u'1793-Washington.txt',\n",
        " u'1797-Adams.txt',\n",
        " u'1801-Jefferson.txt',\n",
        " u'1805-Jefferson.txt',\n",
        " u'1809-Madison.txt',\n",
        " u'1813-Madison.txt',\n",
        " u'1817-Monroe.txt',\n",
        " u'1821-Monroe.txt',\n",
        " u'1825-Adams.txt',\n",
        " u'1829-Jackson.txt',\n",
        " u'1833-Jackson.txt',\n",
        " u'1837-VanBuren.txt',\n",
        " u'1841-Harrison.txt',\n",
        " u'1845-Polk.txt',\n",
        " u'1849-Taylor.txt',\n",
        " u'1853-Pierce.txt',\n",
        " u'1857-Buchanan.txt',\n",
        " u'1861-Lincoln.txt',\n",
        " u'1865-Lincoln.txt',\n",
        " u'1869-Grant.txt',\n",
        " u'1873-Grant.txt',\n",
        " u'1877-Hayes.txt',\n",
        " u'1881-Garfield.txt',\n",
        " u'1885-Cleveland.txt',\n",
        " u'1889-Harrison.txt',\n",
        " u'1893-Cleveland.txt',\n",
        " u'1897-McKinley.txt',\n",
        " u'1901-McKinley.txt',\n",
        " u'1905-Roosevelt.txt',\n",
        " u'1909-Taft.txt',\n",
        " u'1913-Wilson.txt',\n",
        " u'1917-Wilson.txt',\n",
        " u'1921-Harding.txt',\n",
        " u'1925-Coolidge.txt',\n",
        " u'1929-Hoover.txt',\n",
        " u'1933-Roosevelt.txt',\n",
        " u'1937-Roosevelt.txt',\n",
        " u'1941-Roosevelt.txt',\n",
        " u'1945-Roosevelt.txt',\n",
        " u'1949-Truman.txt',\n",
        " u'1953-Eisenhower.txt',\n",
        " u'1957-Eisenhower.txt',\n",
        " u'1961-Kennedy.txt',\n",
        " u'1965-Johnson.txt',\n",
        " u'1969-Nixon.txt',\n",
        " u'1973-Nixon.txt',\n",
        " u'1977-Carter.txt',\n",
        " u'1981-Reagan.txt',\n",
        " u'1985-Reagan.txt',\n",
        " u'1989-Bush.txt',\n",
        " u'1993-Clinton.txt',\n",
        " u'1997-Clinton.txt',\n",
        " u'2001-Bush.txt',\n",
        " u'2005-Bush.txt',\n",
        " u'2009-Obama.txt']"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Reading Wine Reviews"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import webtext\n",
      "webtext.fileids()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "[u'firefox.txt',\n",
        " u'grail.txt',\n",
        " u'overheard.txt',\n",
        " u'pirates.txt',\n",
        " u'singles.txt',\n",
        " u'wine.txt']"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# wine reviews corpus\n",
      "text = webtext.raw('wine.txt')\n",
      "print text[:500]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Lovely delicate, fragrant Rhone wine. Polished leather and strawberries. Perhaps a bit dilute, but good for drinking now. ***\n",
        "Liquorice, cherry fruit. Simple and coarse at the finish. **\n",
        "Thin and completely uninspiring. *\n",
        "Rough. No Stars\n",
        "Big, fat, textured Chardonnay - nuts and butterscotch. A slightly odd metallic/cardboard finish, but probably ***\n",
        "A blind tasting, other than the fizz, which included five vintages of Cote Rotie Brune et Blonde from Guigal.\n",
        "Surprisingly young feeling and drinkin\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Tokenization\n",
      "\n",
      "What:  Separate text into units such as sentences or words\n",
      "\n",
      "Why:   Gives structure to previously unstructured text\n",
      "\n",
      "Notes: \n",
      "* Relatively easy with English language text, not easy with some languages"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk import tokenize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Explore the different tokenizers\n",
      "\n",
      "tokenize.sent_tokenize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "<function nltk.tokenize.sent_tokenize>"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# tokenize into sentences\n",
      "sentences = [sent for sent in tokenize.sent_tokenize(text)]\n",
      "sentences[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "[u'Lovely delicate, fragrant Rhone wine.',\n",
        " u'Polished leather and strawberries.',\n",
        " u'Perhaps a bit dilute, but good for drinking now.',\n",
        " u'***\\nLiquorice, cherry fruit.',\n",
        " u'Simple and coarse at the finish.',\n",
        " u'**\\nThin and completely uninspiring.',\n",
        " u'*\\nRough.',\n",
        " u'No Stars\\nBig, fat, textured Chardonnay - nuts and butterscotch.',\n",
        " u'A slightly odd metallic/cardboard finish, but probably ***\\nA blind tasting, other than the fizz, which included five vintages of Cote Rotie Brune et Blonde from Guigal.',\n",
        " u'Surprisingly young feeling and drinking well, but without any great complexity.']"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# tokenize into words\n",
      "tokens = [word for word in tokenize.word_tokenize(text)]\n",
      "tokens[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "[u'Lovely',\n",
        " u'delicate',\n",
        " u',',\n",
        " u'fragrant',\n",
        " u'Rhone',\n",
        " u'wine',\n",
        " u'.',\n",
        " u'Polished',\n",
        " u'leather',\n",
        " u'and']"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# only keep tokens that start with a letter (using regular expressions)\n",
      "import re\n",
      "clean_tokens = [token for token in tokens if re.search(r'^[a-zA-Z]+', token)]\n",
      "clean_tokens[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "[u'Lovely',\n",
        " u'delicate',\n",
        " u'fragrant',\n",
        " u'Rhone',\n",
        " u'wine',\n",
        " u'Polished',\n",
        " u'leather',\n",
        " u'and',\n",
        " u'strawberries',\n",
        " u'Perhaps']"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's look at word frequency\n",
      "from collections import Counter\n",
      "\n",
      "c = Counter(clean_tokens)\n",
      "c.most_common(25)       \n",
      "\n",
      "# You will see a mixed case words, stop words?, singular, plurals?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "[(u'a', 821),\n",
        " (u'and', 786),\n",
        " (u'the', 706),\n",
        " (u'of', 482),\n",
        " (u'but', 474),\n",
        " (u'I', 390),\n",
        " (u'it', 318),\n",
        " (u'to', 306),\n",
        " (u'fruit', 291),\n",
        " (u'with', 259),\n",
        " (u'good', 250),\n",
        " (u'A', 230),\n",
        " (u'this', 227),\n",
        " (u'wine', 226),\n",
        " (u'bit', 217),\n",
        " (u'in', 212),\n",
        " (u'Very', 211),\n",
        " (u'quite', 204),\n",
        " (u'is', 189),\n",
        " (u'Top', 182),\n",
        " (u'very', 169),\n",
        " (u'that', 152),\n",
        " (u'nose', 150),\n",
        " (u'touch', 146),\n",
        " (u'for', 145)]"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Stemming\n",
      "\n",
      "What:  Reduce a word to its base/stem/root form\n",
      "\n",
      "Why:   Often makes sense to treat multiple word forms the same way\n",
      "\n",
      "Notes: \n",
      "* Uses a \"simple\" and fast rule-based approach\n",
      "* Output can be undesirable for irregular words\n",
      "* Stemmed words are usually not shown to users (used for analysis/indexing)\n",
      "* Some search engines treat words with the same stem as synonyms"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk import stem"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stemmer = stem.SnowballStemmer('english')\n",
      "\n",
      "# example stemming\n",
      "print stemmer.stem('charge')\n",
      "print stemmer.stem('charging')\n",
      "print stemmer.stem('charged')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "charg\n",
        "charg\n",
        "charg\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# stem the tokens\n",
      "stemmed_tokens = [stemmer.stem(t) for t in clean_tokens]\n",
      "stemmed_tokens[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "[u'love',\n",
        " u'delic',\n",
        " u'fragrant',\n",
        " u'rhone',\n",
        " u'wine',\n",
        " u'polish',\n",
        " u'leather',\n",
        " u'and',\n",
        " u'strawberri',\n",
        " u'perhap']"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Lemmatization\n",
      "\n",
      "What:  Derive the canonical form ('lemma') of a word\n",
      "\n",
      "Why:   Can be better than stemming\n",
      "\n",
      "Notes: Uses a dictionary-based approach (slower than stemming)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lemmatizer = stem.WordNetLemmatizer()\n",
      "\n",
      "# example stemming\n",
      "print lemmatizer.lemmatize('charges')\n",
      "print lemmatizer.lemmatize('charging')\n",
      "print lemmatizer.lemmatize('charged')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "charge\n",
        "charging\n",
        "charged\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stem.WordNetLemmatizer?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# stem the tokens\n",
      "stemmed_tokens = [lemmatizer.lemmatize(t) for t in clean_tokens]\n",
      "stemmed_tokens[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "[u'Lovely',\n",
        " u'delicate',\n",
        " u'fragrant',\n",
        " u'Rhone',\n",
        " u'wine',\n",
        " u'Polished',\n",
        " u'leather',\n",
        " u'and',\n",
        " u'strawberry',\n",
        " u'Perhaps']"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Stopword Removal\n",
      "\n",
      "What:  Remove common words that will likely appear in any text\n",
      "\n",
      "Why:   They don't tell you much about your text"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# view the list of stopwords\n",
      "stopwords = nltk.corpus.stopwords.words('english')\n",
      "sorted(stopwords)[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "[u'a',\n",
        " u'about',\n",
        " u'above',\n",
        " u'after',\n",
        " u'again',\n",
        " u'against',\n",
        " u'all',\n",
        " u'am',\n",
        " u'an',\n",
        " u'and']"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# stem the stopwords\n",
      "stemmed_stops = [stemmer.stem(t) for t in stopwords]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# remove stopwords from stemmed tokens\n",
      "stemmed_tokens_no_stop = [stemmer.stem(t) for t in stemmed_tokens if stemmer.stem(t) not in stopwords]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's look at word frequency\n",
      "from collections import Counter\n",
      "\n",
      "c = Counter(stemmed_tokens_no_stop)\n",
      "c.most_common(25) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 33,
       "text": [
        "[(u'veri', 380),\n",
        " (u'good', 363),\n",
        " (u'fruit', 306),\n",
        " (u'quit', 303),\n",
        " (u'wine', 286),\n",
        " (u'bit', 217),\n",
        " (u'top', 215),\n",
        " (u'nice', 211),\n",
        " (u'love', 168),\n",
        " (u'touch', 163),\n",
        " (u'nose', 151),\n",
        " (u'dri', 147),\n",
        " (u'drink', 144),\n",
        " (u'bare', 141),\n",
        " (u'rather', 133),\n",
        " (u'balanc', 120),\n",
        " (u'palat', 115),\n",
        " (u'rich', 114),\n",
        " (u'fine', 106),\n",
        " (u'seem', 100),\n",
        " (u'still', 92),\n",
        " (u'complex', 90),\n",
        " (u'perhap', 88),\n",
        " (u\"n't\", 87),\n",
        " (u'bottl', 87)]"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def zipf_plot(text):\n",
      "    '''Creates a log-log plot of word frequency vs word rank.\n",
      "    \"a function to process a large text and plot word frequency against word rank using pylab.plot\"\n",
      "\n",
      "    Precondition: 'text' contains a list of words.\n",
      "    '''\n",
      "    fdist = nltk.FreqDist(text)\n",
      "    \n",
      "    # figured out pylab.plot syntax, sadly, from random googling\n",
      "    plt.plot(\n",
      "            range(1, fdist.B() + 1),      # x-axis: word rank\n",
      "            fdist.values()                # y-axis: word count\n",
      "            )   \n",
      "    plt.xscale('log')\n",
      "    plt.yscale('log')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.FreqDist(stemmed_tokens_no_stop)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "FreqDist({u'veri': 380, u'good': 363, u'fruit': 306, u'quit': 303, u'wine': 286, u'bit': 217, u'top': 215, u'nice': 211, u'love': 168, u'touch': 163, ...})"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "zipf_plot(stemmed_tokens_no_stop)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEHCAYAAABCwJb2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npEJQ9AkhpEJQ9AkhpEL8P8ydO3/ElZx5AAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x109ff41d0>"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Classify Names as Male or Female"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import names\n",
      "import random"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "names.fileids()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 38,
       "text": [
        "[u'female.txt', u'male.txt']"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "names.words('male.txt')[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "[u'Aamir',\n",
        " u'Aaron',\n",
        " u'Abbey',\n",
        " u'Abbie',\n",
        " u'Abbot',\n",
        " u'Abbott',\n",
        " u'Abby',\n",
        " u'Abdel',\n",
        " u'Abdul',\n",
        " u'Abdulkarim']"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "names.words('female.txt')[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "[u'Abagael',\n",
        " u'Abagail',\n",
        " u'Abbe',\n",
        " u'Abbey',\n",
        " u'Abbi',\n",
        " u'Abbie',\n",
        " u'Abby',\n",
        " u'Abigael',\n",
        " u'Abigail',\n",
        " u'Abigale']"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "names_data = [(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')] "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "random.shuffle(names_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "names_data[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 43,
       "text": [
        "[(u'Rakel', 'female'),\n",
        " (u'Frederica', 'female'),\n",
        " (u'Yetty', 'female'),\n",
        " (u'Aurelia', 'female'),\n",
        " (u'Dennis', 'male'),\n",
        " (u'Jae', 'male'),\n",
        " (u'Carly', 'female'),\n",
        " (u'Colleen', 'female'),\n",
        " (u'Benedikta', 'female'),\n",
        " (u'Gav', 'male')]"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gender_features(word):\n",
      "    features = {}\n",
      "    features['last_letter'] = word[-1]\n",
      "    features['first_letter'] = word[0]\n",
      "    features['length'] = len(word)\n",
      "    return features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gender_features('Ricky')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 73,
       "text": [
        "{'first_letter': 'R', 'last_letter': 'y', 'length': 5}"
       ]
      }
     ],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.classify import apply_features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set = apply_features(gender_features, names_data[500:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 75
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_set = apply_features(gender_features, names_data[:500])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 76
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 77,
       "text": [
        "[({'first_letter': u'L', 'length': 7, 'last_letter': u'n'}, 'female'), ({'first_letter': u'J', 'length': 6, 'last_letter': u'e'}, 'female'), ...]"
       ]
      }
     ],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = nltk.NaiveBayesClassifier.train(train_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Score the Accuracy of the names classifier we built\n",
      "nltk.classify.accuracy(clf, test_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 79,
       "text": [
        "0.772"
       ]
      }
     ],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.show_most_informative_features(20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Most Informative Features\n",
        "             last_letter = u'a'           female : male   =     37.1 : 1.0\n",
        "             last_letter = u'k'             male : female =     33.0 : 1.0\n",
        "             last_letter = u'f'             male : female =     16.5 : 1.0\n",
        "             last_letter = u'p'             male : female =     16.3 : 1.0\n",
        "             last_letter = u'v'             male : female =     10.4 : 1.0\n",
        "             last_letter = u'd'             male : female =      9.6 : 1.0\n",
        "             last_letter = u'o'             male : female =      8.4 : 1.0\n",
        "             last_letter = u'm'             male : female =      7.9 : 1.0\n",
        "             last_letter = u'r'             male : female =      6.7 : 1.0\n",
        "             last_letter = u'g'             male : female =      5.4 : 1.0\n",
        "             last_letter = u'w'             male : female =      4.7 : 1.0\n",
        "            first_letter = u'W'             male : female =      4.5 : 1.0\n",
        "             last_letter = u't'             male : female =      4.3 : 1.0\n",
        "             last_letter = u's'             male : female =      4.2 : 1.0\n",
        "             last_letter = u'j'             male : female =      3.9 : 1.0\n",
        "             last_letter = u'z'             male : female =      3.9 : 1.0\n",
        "             last_letter = u'i'           female : male   =      3.8 : 1.0\n",
        "             last_letter = u'b'             male : female =      3.7 : 1.0\n",
        "            first_letter = u'Q'             male : female =      3.1 : 1.0\n",
        "            first_letter = u'X'             male : female =      2.8 : 1.0\n"
       ]
      }
     ],
     "prompt_number": 80
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Movie Reviews"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!ls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "nltk-lab.ipynb\r\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's classify Movie Reviews\n",
      "df = pd.read_csv('../data/movie-reviews-dataset.tsv', sep='\\t', names=['rating', 'review'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>rating</th>\n",
        "      <th>review</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> negative</td>\n",
        "      <td>                 simplistic , silly and tedious . </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> negative</td>\n",
        "      <td> it's so laddish and juvenile , only teenage bo...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> negative</td>\n",
        "      <td>       a sentimental mess that never rings true . </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> negative</td>\n",
        "      <td> completely awful iranian drama . . . as much f...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> negative</td>\n",
        "      <td>  \" analyze that \" is one of those crass , cont...</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 55,
       "text": [
        "     rating                                             review\n",
        "0  negative                  simplistic , silly and tedious . \n",
        "1  negative  it's so laddish and juvenile , only teenage bo...\n",
        "2  negative        a sentimental mess that never rings true . \n",
        "3  negative  completely awful iranian drama . . . as much f...\n",
        "4  negative   \" analyze that \" is one of those crass , cont..."
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.values[0][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 56,
       "text": [
        "'negative'"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "def stem_review(category, text):\n",
      "    # Let's use Porter Stemmer, but you can use Snowball Stemmer or Lammatizer\n",
      "    stemmer = stem.PorterStemmer()\n",
      "    stopwords = nltk.corpus.stopwords.words('english')\n",
      "\n",
      "    # Tokenize the words\n",
      "    words = tokenize.word_tokenize(text)\n",
      "    words = [token for token in words if re.search(r'^[a-zA-Z]+', token)]\n",
      "    \n",
      "    # Stem the words\n",
      "    #stem_words = [stemmer.stem(word) for word in words if word not in stopwords]\n",
      "    return (words, category)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 147
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "documents = []\n",
      "for cat, review in df.values:\n",
      "    review = unicode(review, 'utf8')\n",
      "    review.encode('utf8','ignore')\n",
      "    documents.append(stem_review(cat, review))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 148
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "documents[0:4]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 149,
       "text": [
        "[([u'Go',\n",
        "   u'until',\n",
        "   u'jurong',\n",
        "   u'point',\n",
        "   u'crazy..',\n",
        "   u'Available',\n",
        "   u'only',\n",
        "   u'in',\n",
        "   u'bugis',\n",
        "   u'n',\n",
        "   u'great',\n",
        "   u'world',\n",
        "   u'la',\n",
        "   u'e',\n",
        "   u'buffet',\n",
        "   u'Cine',\n",
        "   u'there',\n",
        "   u'got',\n",
        "   u'amore',\n",
        "   u'wat'],\n",
        "  'ham'),\n",
        " ([u'Ok', u'lar', u'Joking', u'wif', u'u', u'oni'], 'ham'),\n",
        " ([u'Free',\n",
        "   u'entry',\n",
        "   u'in',\n",
        "   u'a',\n",
        "   u'wkly',\n",
        "   u'comp',\n",
        "   u'to',\n",
        "   u'win',\n",
        "   u'FA',\n",
        "   u'Cup',\n",
        "   u'final',\n",
        "   u'tkts',\n",
        "   u'May',\n",
        "   u'Text',\n",
        "   u'FA',\n",
        "   u'to',\n",
        "   u'to',\n",
        "   u'receive',\n",
        "   u'entry',\n",
        "   u'question',\n",
        "   u'std',\n",
        "   u'txt',\n",
        "   u'rate',\n",
        "   u'T',\n",
        "   u'C',\n",
        "   u'apply'],\n",
        "  'spam'),\n",
        " ([u'U',\n",
        "   u'dun',\n",
        "   u'say',\n",
        "   u'so',\n",
        "   u'early',\n",
        "   u'hor',\n",
        "   u'U',\n",
        "   u'c',\n",
        "   u'already',\n",
        "   u'then',\n",
        "   u'say'],\n",
        "  'ham')]"
       ]
      }
     ],
     "prompt_number": 149
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Number of Reviews\n",
      "len(documents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 150,
       "text": [
        "5572"
       ]
      }
     ],
     "prompt_number": 150
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Function to convert document words into \"Feature\" Dictionary required by NLTK Classifier\n",
      "def document_features(document_words): # [_document-classify-extractor]\n",
      "    features = {}\n",
      "    for word in document_words:\n",
      "        features[word] = True\n",
      "    return features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 151
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "document_features([u'simplistic', u'tedious'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 152,
       "text": [
        "{u'simplistic': True, u'tedious': True}"
       ]
      }
     ],
     "prompt_number": 152
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "featuresets = [(document_features(d), c) for (d,c) in documents]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 153
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "featuresets[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 154,
       "text": [
        "({u'Available': True,\n",
        "  u'Cine': True,\n",
        "  u'Go': True,\n",
        "  u'amore': True,\n",
        "  u'buffet': True,\n",
        "  u'bugis': True,\n",
        "  u'crazy..': True,\n",
        "  u'e': True,\n",
        "  u'got': True,\n",
        "  u'great': True,\n",
        "  u'in': True,\n",
        "  u'jurong': True,\n",
        "  u'la': True,\n",
        "  u'n': True,\n",
        "  u'only': True,\n",
        "  u'point': True,\n",
        "  u'there': True,\n",
        "  u'until': True,\n",
        "  u'wat': True,\n",
        "  u'world': True},\n",
        " 'ham')"
       ]
      }
     ],
     "prompt_number": 154
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Shuffle the data before doing the training\n",
      "random.shuffle(featuresets)\n",
      "\n",
      "train_set, test_set = featuresets[:5572/2], featuresets[5572/2:]\n",
      "\n",
      "# Build the NaiveBayes Classifier\n",
      "classifier = nltk.NaiveBayesClassifier.train(train_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 155
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.classify.accuracy(classifier, test_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 156,
       "text": [
        "0.9296482412060302"
       ]
      }
     ],
     "prompt_number": 156
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print nltk.classify.accuracy(classifier, test_set) \n",
      "classifier.show_most_informative_features(20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.929648241206\n",
        "Most Informative Features\n",
        "                    FREE = True             spam : ham    =    214.6 : 1.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "                     Txt = True             spam : ham    =    165.5 : 1.0\n",
        "                  mobile = True             spam : ham    =     93.7 : 1.0\n",
        "                      To = True             spam : ham    =     92.0 : 1.0\n",
        "                    STOP = True             spam : ham    =     87.9 : 1.0\n",
        "                    Text = True             spam : ham    =     87.9 : 1.0\n",
        "                landline = True             spam : ham    =     75.6 : 1.0\n",
        "                   Reply = True             spam : ham    =     62.5 : 1.0\n",
        "                 service = True             spam : ham    =     62.5 : 1.0\n",
        "                   apply = True             spam : ham    =     59.3 : 1.0\n",
        "                     txt = True             spam : ham    =     53.8 : 1.0\n",
        "                 receive = True             spam : ham    =     51.1 : 1.0\n",
        "                   video = True             spam : ham    =     47.0 : 1.0\n",
        "                   award = True             spam : ham    =     47.0 : 1.0\n",
        "                    draw = True             spam : ham    =     41.2 : 1.0\n",
        "                   shows = True             spam : ham    =     40.5 : 1.0\n",
        "                    Call = True             spam : ham    =     40.0 : 1.0\n",
        "                    line = True             spam : ham    =     37.7 : 1.0\n",
        "                  latest = True             spam : ham    =     35.6 : 1.0\n",
        "                  pounds = True             spam : ham    =     34.7 : 1.0\n"
       ]
      }
     ],
     "prompt_number": 157
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Term Frequency - Inverse Document Frequency (TF-IDF)\n",
      "\n",
      "What:  Computes \"relative frequency\" that a word appears in a document compared to its frequency across all documents\n",
      "\n",
      "Why:   More useful than \"term frequency\" for identifying \"important\" words in each document (high frequency in that document, low frequency in other documents)\n",
      "\n",
      "Notes: Used for search engine scoring, text summarization, document clustering\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "categories = []\n",
      "reviews = []\n",
      "for words, cat in documents:\n",
      "    reviews.append(' '.join(words))\n",
      "    categories.append(cat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "tfidf = TfidfVectorizer()\n",
      "X = tfidf.fit_transform(reviews).toarray()\n",
      "lbl = LabelEncoder()\n",
      "y = lbl.fit_transform(categories)\n",
      "tfidf.get_feature_names()[-10:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 68,
       "text": [
        "[u'ziyi',\n",
        " u'zoe',\n",
        " u'zombi',\n",
        " u'zombie',\n",
        " u'zone',\n",
        " u'zooland',\n",
        " u'zoom',\n",
        " u'zucker',\n",
        " u'zwick',\n",
        " u'zzzzzzzzz']"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
      "\n",
      "sk_clf = GaussianNB()\n",
      "sk_clf.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 69,
       "text": [
        "GaussianNB()"
       ]
      }
     ],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sk_clf.score(X_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 70,
       "text": [
        "0.64966241560390092"
       ]
      }
     ],
     "prompt_number": 70
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Your Turn - Build a Classifier to identify Spam from Ham\n",
      "\n",
      "* Use Dataset in labs/data/SpamCollection.tsv"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's classify emails\n",
      "df = pd.read_csv('../data/SpamCollection.tsv', sep='\\t', names=['mail_type', 'text'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 158
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>mail_type</th>\n",
        "      <th>text</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>  ham</td>\n",
        "      <td> Go until jurong point, crazy.. Available only ...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>  ham</td>\n",
        "      <td>                     Ok lar... Joking wif u oni...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> spam</td>\n",
        "      <td> Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>  ham</td>\n",
        "      <td> U dun say so early hor... U c already then say...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>  ham</td>\n",
        "      <td> Nah I don't think he goes to usf, he lives aro...</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 161,
       "text": [
        "  mail_type                                               text\n",
        "0       ham  Go until jurong point, crazy.. Available only ...\n",
        "1       ham                      Ok lar... Joking wif u oni...\n",
        "2      spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
        "3       ham  U dun say so early hor... U c already then say...\n",
        "4       ham  Nah I don't think he goes to usf, he lives aro..."
       ]
      }
     ],
     "prompt_number": 161
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "emails = []\n",
      "for cat, review in df.values:\n",
      "    review = unicode(review, 'utf8')\n",
      "    review.encode('utf8','ignore')\n",
      "    emails.append(stem_review(cat, review))\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 175
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from nltk.probability import FreqDist\n",
      "from nltk.classify import SklearnClassifier\n",
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "from sklearn.feature_selection import SelectKBest, chi2\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.pipeline import Pipeline\n",
      "\n",
      "pipeline = Pipeline([('tfidf', TfidfTransformer()),\n",
      "                     ('chi2', SelectKBest(chi2, k=1000)),\n",
      "                     ('nb', MultinomialNB())])\n",
      "clf = SklearnClassifier(pipeline)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'str' object has no attribute 'iteritems'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-183-376e2c5d58c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mneg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmail_type\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'spam'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0madd_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlab\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlab\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mclassif\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ham'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0madd_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'spam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0ml_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassif\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify_many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/jeremiah/anaconda/lib/python2.7/site-packages/nltk/classify/scikitlearn.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, labeled_featuresets)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mizip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlabeled_featuresets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/jeremiah/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/dict_vectorizer.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \"\"\"\n\u001b[1;32m    140\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tosequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/jeremiah/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/dict_vectorizer.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%s%s%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseparator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/jeremiah/anaconda/lib/python2.7/site-packages/sklearn/externals/six.pyc\u001b[0m in \u001b[0;36miteritems\u001b[0;34m(d, **kw)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;34m\"\"\"Return an iterator over the (key, value) pairs of a dictionary.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_iteritems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0miterlists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'iteritems'"
       ]
      }
     ],
     "prompt_number": 183
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 181,
       "text": [
        "0     Go until jurong point, crazy.. Available only ...\n",
        "1                         Ok lar... Joking wif u oni...\n",
        "3     U dun say so early hor... U c already then say...\n",
        "4     Nah I don't think he goes to usf, he lives aro...\n",
        "6     Even my brother is not like to speak with me. ...\n",
        "7     As per your request 'Melle Melle (Oru Minnamin...\n",
        "10    I'm gonna be home soon and i don't want to tal...\n",
        "13    I've been searching for the right words to tha...\n",
        "14                  I HAVE A DATE ON SUNDAY WITH WILL!!\n",
        "16                           Oh k...i'm watching here:)\n",
        "17    Eh u remember how 2 spell his name... Yes i di...\n",
        "18    Fine if that\u0092s the way u feel. That\u0092s the way ...\n",
        "20            Is that seriously how you spell his name?\n",
        "21      I\u2018m going to try for 2 months ha ha only joking\n",
        "22    So \u00fc pay first lar... Then when is da stock co...\n",
        "...\n",
        "5555    Yeh. Indians was nice. Tho it did kane me off ...\n",
        "5556    Yes i have. So that's why u texted. Pshew...mi...\n",
        "5557    No. I meant the calculation is the same. That ...\n",
        "5558                               Sorry, I'll call later\n",
        "5559    if you aren't here in the next  &lt;#&gt;  hou...\n",
        "5560                    Anything lor. Juz both of us lor.\n",
        "5561    Get me out of this dump heap. My mom decided t...\n",
        "5562    Ok lor... Sony ericsson salesman... I ask shuh...\n",
        "5563                                  Ard 6 like dat lor.\n",
        "5564    Why don't you wait 'til at least wednesday to ...\n",
        "5565                                         Huh y lei...\n",
        "5568                 Will \u00fc b going to esplanade fr home?\n",
        "5569    Pity, * was in mood for that. So...any other s...\n",
        "5570    The guy did some bitching but I acted like i'd...\n",
        "5571                           Rofl. Its true to its name\n",
        "Name: text, Length: 4825, dtype: object"
       ]
      }
     ],
     "prompt_number": 181
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}