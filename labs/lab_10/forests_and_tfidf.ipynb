{
 "metadata": {
  "name": "",
<<<<<<< HEAD
  "signature": "sha256:b8ebf91d2663781b4ba9ca45b338e6b870195516f21e4f52c6fcf9439eeeddbc"
=======
  "signature": "sha256:e5f4646f088251c5bb5aa3dd7ef1418aa0f09b6f459abc8d1045599453dbc8c3"
>>>>>>> 5ff02bdacae2a66fc9c21becf1d686e529da36c3
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Using tf-idf and random forests"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Newsgroups Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will take a look at some of the twenty newsgroups dataset, another common dataset for classification. Note that the data is fetched from."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import fetch_20newsgroups\n",
      "\n",
      "# We will use four of the twenty newsgroups\n",
      "categories = ['alt.atheism',\n",
      "              'talk.religion.misc',\n",
      "              'comp.graphics',\n",
      "              'sci.space']\n",
      "\n",
      "twenty_train_subset = fetch_20newsgroups(subset='train', categories=categories)\n",
      "twenty_test_subset = fetch_20newsgroups(subset='test', categories=categories)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:sklearn.datasets.twenty_newsgroups:Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we have lists of messages (as strings) in the `.data` members."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "datasets.get_data_home()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "'/Users/jeremiah/scikit_learn_data'"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Features from text"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here are some ways to generate features from the text:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Count Vectorizer\n",
      "Count Vectorizer is the easiest text processing utility to understand, it simply counts the occurances of non-stopwords. (what is a stopword?)<br>\n",
      "First, let's check out our data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print twenty_train_subset.data[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "From: rych@festival.ed.ac.uk (R Hawkes)\n",
        "Subject: 3DS: Where did all the texture rules go?\n",
        "Lines: 21\n",
        "\n",
        "Hi,\n",
        "\n",
        "I've noticed that if you only save a model (with all your mapping planes\n",
        "positioned carefully) to a .3DS file that when you reload it after restarting\n",
        "3DS, they are given a default position and orientation.  But if you save\n",
        "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
        "know why this information is not stored in the .3DS file?  Nothing is\n",
        "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
        "I'd like to be able to read the texture rule information, does anyone have \n",
        "the format for the .PRJ file?\n",
        "\n",
        "Is the .CEL file format available from somewhere?\n",
        "\n",
        "Rych\n",
        "\n",
        "======================================================================\n",
        "Rycharde Hawkes\t\t\t\temail: rych@festival.ed.ac.uk\n",
        "Virtual Environment Laboratory\n",
        "Dept. of Psychology\t\t\tTel  : +44 31 650 3426\n",
        "Univ. of Edinburgh\t\t\tFax  : +44 31 667 0150\n",
        "======================================================================\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "vectorizer = CountVectorizer(stop_words='english')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "CountVectorizer(analyzer=u'word', binary=False, charset=None,\n",
        "        charset_error=None, decode_error=u'strict',\n",
        "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
        "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
        "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
        "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
        "        tokenizer=None, vocabulary=None)"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "one_message = vectorizer.fit_transform(twenty_train_subset.data[0:3]).todense().tolist()[0]\n",
      "zip(vectorizer.get_feature_names(),one_message)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "[(u'0150', 1L),\n",
        " (u'02', 0L),\n",
        " (u'020359', 0L),\n",
        " (u'04', 0L),\n",
        " (u'07', 0L),\n",
        " (u'08', 0L),\n",
        " (u'15', 0L),\n",
        " (u'19', 0L),\n",
        " (u'1970', 0L),\n",
        " (u'1993', 0L),\n",
        " (u'1993apr19', 0L),\n",
        " (u'20', 0L),\n",
        " (u'20apr199301460499', 0L),\n",
        " (u'21', 1L),\n",
        " (u'23', 0L),\n",
        " (u'2400x4', 0L),\n",
        " (u'245', 0L),\n",
        " (u'26996', 0L),\n",
        " (u'31', 2L),\n",
        " (u'3205', 0L),\n",
        " (u'3426', 1L),\n",
        " (u'3ds', 4L),\n",
        " (u'4366', 0L),\n",
        " (u'44', 2L),\n",
        " (u'55', 0L),\n",
        " (u'604', 0L),\n",
        " (u'650', 1L),\n",
        " (u'667', 1L),\n",
        " (u'_perijoves_', 0L),\n",
        " (u'able', 1L),\n",
        " (u'ac', 2L),\n",
        " (u'access', 0L),\n",
        " (u'actually', 0L),\n",
        " (u'almanac', 0L),\n",
        " (u'apoapsis', 0L),\n",
        " (u'apr', 0L),\n",
        " (u'article', 0L),\n",
        " (u'available', 1L),\n",
        " (u'b645zaw', 0L),\n",
        " (u'backing', 0L),\n",
        " (u'barring', 0L),\n",
        " (u'bc', 0L),\n",
        " (u'biblical', 0L),\n",
        " (u'brader', 0L),\n",
        " (u'british', 0L),\n",
        " (u'bunch', 0L),\n",
        " (u'ca', 0L),\n",
        " (u'canada', 0L),\n",
        " (u'carefully', 1L),\n",
        " (u'carried', 0L),\n",
        " (u'cel', 1L),\n",
        " (u'central', 0L),\n",
        " (u'centuries', 0L),\n",
        " (u'children', 0L),\n",
        " (u'circa', 0L),\n",
        " (u'cites', 0L),\n",
        " (u'columbia', 0L),\n",
        " (u'com', 0L),\n",
        " (u'comet', 0L),\n",
        " (u'contrary', 0L),\n",
        " (u'corruption', 0L),\n",
        " (u'couldn', 0L),\n",
        " (u'default', 1L),\n",
        " (u'delusional', 0L),\n",
        " (u'demonstrating', 0L),\n",
        " (u'dept', 1L),\n",
        " (u'deranged', 0L),\n",
        " (u'did', 1L),\n",
        " (u'does', 2L),\n",
        " (u'ed', 2L),\n",
        " (u'edinburgh', 1L),\n",
        " (u'edu', 0L),\n",
        " (u'email', 1L),\n",
        " (u'enclosed', 0L),\n",
        " (u'environment', 1L),\n",
        " (u'evidence', 0L),\n",
        " (u'evil', 0L),\n",
        " (u'explicitly', 1L),\n",
        " (u'f208', 0L),\n",
        " (u'fanatic', 0L),\n",
        " (u'fax', 1L),\n",
        " (u'feb', 0L),\n",
        " (u'festival', 2L),\n",
        " (u'fidonet', 0L),\n",
        " (u'figure', 0L),\n",
        " (u'file', 6L),\n",
        " (u'fisher', 0L),\n",
        " (u'folks', 0L),\n",
        " (u'format', 2L),\n",
        " (u'frog', 0L),\n",
        " (u'fruitcakes', 0L),\n",
        " (u'given', 1L),\n",
        " (u'got', 0L),\n",
        " (u'gotten', 0L),\n",
        " (u'gt', 0L),\n",
        " (u'hawkes', 2L),\n",
        " (u'hi', 1L),\n",
        " (u'hisse', 0L),\n",
        " (u'holocaust', 0L),\n",
        " (u'home', 0L),\n",
        " (u'information', 2L),\n",
        " (u'internet', 0L),\n",
        " (u'island', 0L),\n",
        " (u'jg', 0L),\n",
        " (u'jgarland', 0L),\n",
        " (u'jim', 0L),\n",
        " (u'jones', 0L),\n",
        " (u'jupiter', 0L),\n",
        " (u'just', 0L),\n",
        " (u'kean', 0L),\n",
        " (u'ken', 0L),\n",
        " (u'killed', 0L),\n",
        " (u'kmcvay', 0L),\n",
        " (u'know', 1L),\n",
        " (u'koresh', 0L),\n",
        " (u'laboratory', 1L),\n",
        " (u'ladysmith', 0L),\n",
        " (u'language', 0L),\n",
        " (u'learned', 0L),\n",
        " (u'like', 1L),\n",
        " (u'lines', 1L),\n",
        " (u'lot', 0L),\n",
        " (u'mail', 0L),\n",
        " (u'mania', 0L),\n",
        " (u'manual', 1L),\n",
        " (u'mapping', 1L),\n",
        " (u'mark', 0L),\n",
        " (u'mb', 0L),\n",
        " (u'mcvay', 0L),\n",
        " (u'mean', 0L),\n",
        " (u'message', 0L),\n",
        " (u'messenger', 0L),\n",
        " (u'model', 1L),\n",
        " (u'msb', 0L),\n",
        " (u'msged', 0L),\n",
        " (u'mun', 0L),\n",
        " (u'n103', 0L),\n",
        " (u'neccessary', 0L),\n",
        " (u'newtout', 0L),\n",
        " (u'nope', 0L),\n",
        " (u'noticed', 1L),\n",
        " (u'old', 0L),\n",
        " (u'oneb', 0L),\n",
        " (u'orbit', 0L),\n",
        " (u'org', 0L),\n",
        " (u'organization', 0L),\n",
        " (u'orientation', 2L),\n",
        " (u'p201', 0L),\n",
        " (u'perew', 0L),\n",
        " (u'periapsis', 0L),\n",
        " (u'perijove', 0L),\n",
        " (u'planes', 1L),\n",
        " (u'position', 1L),\n",
        " (u'positioned', 1L),\n",
        " (u'positions', 1L),\n",
        " (u'preserved', 1L),\n",
        " (u'prj', 3L),\n",
        " (u'psychology', 1L),\n",
        " (u'public', 0L),\n",
        " (u'read', 1L),\n",
        " (u'reload', 1L),\n",
        " (u'restarting', 1L),\n",
        " (u'rotten', 0L),\n",
        " (u'rule', 1L),\n",
        " (u'rules', 2L),\n",
        " (u'rych', 3L),\n",
        " (u'rycharde', 1L),\n",
        " (u'ryugen', 0L),\n",
        " (u'said', 1L),\n",
        " (u'salute', 0L),\n",
        " (u'satisfy', 0L),\n",
        " (u'save', 2L),\n",
        " (u'saving', 1L),\n",
        " (u'say', 0L),\n",
        " (u'says', 0L),\n",
        " (u'sco', 0L),\n",
        " (u'sender', 0L),\n",
        " (u'serving', 0L),\n",
        " (u'simply', 0L),\n",
        " (u'sorry', 0L),\n",
        " (u'sq', 0L),\n",
        " (u'stephen', 0L),\n",
        " (u'stored', 1L),\n",
        " (u'subject', 1L),\n",
        " (u'sure', 0L),\n",
        " (u'surprised', 0L),\n",
        " (u'talking', 0L),\n",
        " (u'tape', 0L),\n",
        " (u'tel', 1L),\n",
        " (u'temporary', 0L),\n",
        " (u'texture', 3L),\n",
        " (u'things', 0L),\n",
        " (u'thought', 0L),\n",
        " (u'time', 0L),\n",
        " (u'ucs', 0L),\n",
        " (u'uk', 2L),\n",
        " (u'univ', 1L),\n",
        " (u'unlikely', 0L),\n",
        " (u'used', 0L),\n",
        " (u'usenet', 0L),\n",
        " (u'uta', 0L),\n",
        " (u'utarlg', 0L),\n",
        " (u'v32', 0L),\n",
        " (u'vancouver', 0L),\n",
        " (u've', 1L),\n",
        " (u'virtual', 1L),\n",
        " (u'writes', 0L),\n",
        " (u'xenix', 0L),\n",
        " (u'z1', 0L)]"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This makes a matrix of word counts, where each row is a document and each column is the word, the cell matrix[document, word] contains the count of word in document.\n",
      "<br><br>\n",
      "Now try this with the whole training subset:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train = vectorizer.fit_transform(twenty_train_subset.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "<2034x33815 sparse matrix of type '<type 'numpy.int64'>'\n",
        "\twith 233470 stored elements in Compressed Sparse Row format>"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By default this returns a sparse matrix, which will save memory.\n",
      "\n",
      "Also, notice our stop words:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer.get_stop_words()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "frozenset({'a',\n",
        "           'about',\n",
        "           'above',\n",
        "           'across',\n",
        "           'after',\n",
        "           'afterwards',\n",
        "           'again',\n",
        "           'against',\n",
        "           'all',\n",
        "           'almost',\n",
        "           'alone',\n",
        "           'along',\n",
        "           'already',\n",
        "           'also',\n",
        "           'although',\n",
        "           'always',\n",
        "           'am',\n",
        "           'among',\n",
        "           'amongst',\n",
        "           'amoungst',\n",
        "           'amount',\n",
        "           'an',\n",
        "           'and',\n",
        "           'another',\n",
        "           'any',\n",
        "           'anyhow',\n",
        "           'anyone',\n",
        "           'anything',\n",
        "           'anyway',\n",
        "           'anywhere',\n",
        "           'are',\n",
        "           'around',\n",
        "           'as',\n",
        "           'at',\n",
        "           'back',\n",
        "           'be',\n",
        "           'became',\n",
        "           'because',\n",
        "           'become',\n",
        "           'becomes',\n",
        "           'becoming',\n",
        "           'been',\n",
        "           'before',\n",
        "           'beforehand',\n",
        "           'behind',\n",
        "           'being',\n",
        "           'below',\n",
        "           'beside',\n",
        "           'besides',\n",
        "           'between',\n",
        "           'beyond',\n",
        "           'bill',\n",
        "           'both',\n",
        "           'bottom',\n",
        "           'but',\n",
        "           'by',\n",
        "           'call',\n",
        "           'can',\n",
        "           'cannot',\n",
        "           'cant',\n",
        "           'co',\n",
        "           'con',\n",
        "           'could',\n",
        "           'couldnt',\n",
        "           'cry',\n",
        "           'de',\n",
        "           'describe',\n",
        "           'detail',\n",
        "           'do',\n",
        "           'done',\n",
        "           'down',\n",
        "           'due',\n",
        "           'during',\n",
        "           'each',\n",
        "           'eg',\n",
        "           'eight',\n",
        "           'either',\n",
        "           'eleven',\n",
        "           'else',\n",
        "           'elsewhere',\n",
        "           'empty',\n",
        "           'enough',\n",
        "           'etc',\n",
        "           'even',\n",
        "           'ever',\n",
        "           'every',\n",
        "           'everyone',\n",
        "           'everything',\n",
        "           'everywhere',\n",
        "           'except',\n",
        "           'few',\n",
        "           'fifteen',\n",
        "           'fify',\n",
        "           'fill',\n",
        "           'find',\n",
        "           'fire',\n",
        "           'first',\n",
        "           'five',\n",
        "           'for',\n",
        "           'former',\n",
        "           'formerly',\n",
        "           'forty',\n",
        "           'found',\n",
        "           'four',\n",
        "           'from',\n",
        "           'front',\n",
        "           'full',\n",
        "           'further',\n",
        "           'get',\n",
        "           'give',\n",
        "           'go',\n",
        "           'had',\n",
        "           'has',\n",
        "           'hasnt',\n",
        "           'have',\n",
        "           'he',\n",
        "           'hence',\n",
        "           'her',\n",
        "           'here',\n",
        "           'hereafter',\n",
        "           'hereby',\n",
        "           'herein',\n",
        "           'hereupon',\n",
        "           'hers',\n",
        "           'herself',\n",
        "           'him',\n",
        "           'himself',\n",
        "           'his',\n",
        "           'how',\n",
        "           'however',\n",
        "           'hundred',\n",
        "           'i',\n",
        "           'ie',\n",
        "           'if',\n",
        "           'in',\n",
        "           'inc',\n",
        "           'indeed',\n",
        "           'interest',\n",
        "           'into',\n",
        "           'is',\n",
        "           'it',\n",
        "           'its',\n",
        "           'itself',\n",
        "           'keep',\n",
        "           'last',\n",
        "           'latter',\n",
        "           'latterly',\n",
        "           'least',\n",
        "           'less',\n",
        "           'ltd',\n",
        "           'made',\n",
        "           'many',\n",
        "           'may',\n",
        "           'me',\n",
        "           'meanwhile',\n",
        "           'might',\n",
        "           'mill',\n",
        "           'mine',\n",
        "           'more',\n",
        "           'moreover',\n",
        "           'most',\n",
        "           'mostly',\n",
        "           'move',\n",
        "           'much',\n",
        "           'must',\n",
        "           'my',\n",
        "           'myself',\n",
        "           'name',\n",
        "           'namely',\n",
        "           'neither',\n",
        "           'never',\n",
        "           'nevertheless',\n",
        "           'next',\n",
        "           'nine',\n",
        "           'no',\n",
        "           'nobody',\n",
        "           'none',\n",
        "           'noone',\n",
        "           'nor',\n",
        "           'not',\n",
        "           'nothing',\n",
        "           'now',\n",
        "           'nowhere',\n",
        "           'of',\n",
        "           'off',\n",
        "           'often',\n",
        "           'on',\n",
        "           'once',\n",
        "           'one',\n",
        "           'only',\n",
        "           'onto',\n",
        "           'or',\n",
        "           'other',\n",
        "           'others',\n",
        "           'otherwise',\n",
        "           'our',\n",
        "           'ours',\n",
        "           'ourselves',\n",
        "           'out',\n",
        "           'over',\n",
        "           'own',\n",
        "           'part',\n",
        "           'per',\n",
        "           'perhaps',\n",
        "           'please',\n",
        "           'put',\n",
        "           'rather',\n",
        "           're',\n",
        "           'same',\n",
        "           'see',\n",
        "           'seem',\n",
        "           'seemed',\n",
        "           'seeming',\n",
        "           'seems',\n",
        "           'serious',\n",
        "           'several',\n",
        "           'she',\n",
        "           'should',\n",
        "           'show',\n",
        "           'side',\n",
        "           'since',\n",
        "           'sincere',\n",
        "           'six',\n",
        "           'sixty',\n",
        "           'so',\n",
        "           'some',\n",
        "           'somehow',\n",
        "           'someone',\n",
        "           'something',\n",
        "           'sometime',\n",
        "           'sometimes',\n",
        "           'somewhere',\n",
        "           'still',\n",
        "           'such',\n",
        "           'system',\n",
        "           'take',\n",
        "           'ten',\n",
        "           'than',\n",
        "           'that',\n",
        "           'the',\n",
        "           'their',\n",
        "           'them',\n",
        "           'themselves',\n",
        "           'then',\n",
        "           'thence',\n",
        "           'there',\n",
        "           'thereafter',\n",
        "           'thereby',\n",
        "           'therefore',\n",
        "           'therein',\n",
        "           'thereupon',\n",
        "           'these',\n",
        "           'they',\n",
        "           'thick',\n",
        "           'thin',\n",
        "           'third',\n",
        "           'this',\n",
        "           'those',\n",
        "           'though',\n",
        "           'three',\n",
        "           'through',\n",
        "           'throughout',\n",
        "           'thru',\n",
        "           'thus',\n",
        "           'to',\n",
        "           'together',\n",
        "           'too',\n",
        "           'top',\n",
        "           'toward',\n",
        "           'towards',\n",
        "           'twelve',\n",
        "           'twenty',\n",
        "           'two',\n",
        "           'un',\n",
        "           'under',\n",
        "           'until',\n",
        "           'up',\n",
        "           'upon',\n",
        "           'us',\n",
        "           'very',\n",
        "           'via',\n",
        "           'was',\n",
        "           'we',\n",
        "           'well',\n",
        "           'were',\n",
        "           'what',\n",
        "           'whatever',\n",
        "           'when',\n",
        "           'whence',\n",
        "           'whenever',\n",
        "           'where',\n",
        "           'whereafter',\n",
        "           'whereas',\n",
        "           'whereby',\n",
        "           'wherein',\n",
        "           'whereupon',\n",
        "           'wherever',\n",
        "           'whether',\n",
        "           'which',\n",
        "           'while',\n",
        "           'whither',\n",
        "           'who',\n",
        "           'whoever',\n",
        "           'whole',\n",
        "           'whom',\n",
        "           'whose',\n",
        "           'why',\n",
        "           'will',\n",
        "           'with',\n",
        "           'within',\n",
        "           'without',\n",
        "           'would',\n",
        "           'yet',\n",
        "           'you',\n",
        "           'your',\n",
        "           'yours',\n",
        "           'yourself',\n",
        "           'yourselves'})"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###n-gram\n",
      "The basic idea of n-gram is to take a sequence of objects and make sense of it. Take, for example:<br><br>\n",
      "\n",
      "`I am Sam,`<br>\n",
      "`Sam I am,`<br>\n",
      "`Do you like green eggs and ham?`<br><br>\n",
      "\n",
      "To gram this we will extract all sequences of length `n` like so (for `n=3`):<br><br>\n",
      "`(i,am,sam),(am,sam,sam),(sam,sam,i),(sam,i,am),...`<br><br>\n",
      "\n",
      "scikit gives us the option to use n-grams as features their extraction module:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Include every 1-gram, 2-gram, and 3-gram\n",
      "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
      "X_train = vectorizer.fit_transform(twenty_train_subset.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that this heavily inflates feature set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "<2034x688611 sparse matrix of type '<type 'numpy.int64'>'\n",
        "\twith 1437560 stored elements in Compressed Sparse Row format>"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###TF-IDF"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Additionally, we could use a tf-idf representation, which stands for Term Frequency, Inverse Document Frequency.\n",
      "\n",
      "This value is the product of two intermediate values, the Term Frequency and the Inverse Document Frequency.\n",
      "\n",
      "The Term Frequency is equivalent to the `CountVectorizer` features, the number of times or count that a word appear in the document. This is our most basic representation of text.\n",
      "\n",
      "To establish Inverse Document Frequency, first let's define Document Frequency. This is the percentage of documents that a particular word appears in. For example, the word `the` might appear in 100% of documents, while words like `Syria` would likely have low document frequency. Inverse Document Frequency is simply 1 / Document Frequency (although often the log is also taken).\n",
      "\n",
      "Let, $D$ be the set of all documents:\n",
      "$$\n",
      "idf(word,D) = \\log \\frac{N}{|\\{d \\in D : t \\in d\\}|}\n",
      "$$\n",
      "\n",
      "Notice that this has the neat property that if a word occurs in ALL documents, $idf = 0$ so this naturally controls for stop words \n",
      "\n",
      "So tf-idf is Term Frequency * Inverse Document Frequency, or similar to Term Frequency / Document Frequency. The intuition is that words that have high weight are those that appear a lot in this document and/or appear in very few other documents (somehow unique to this document).\n",
      "\n",
      "Example:\n",
      "Let, $d_1 = $\"i am sam sam i am\", and $d_2 = $ \"i do not like them sam i am\", then:\n",
      "$$\n",
      "tf(like,d_2) = 1\n",
      "$$\n",
      "$$\n",
      "idf(like,D) = \\log \\frac{2}{1} = 0.3010\n",
      "$$\n",
      "$$\n",
      "tfidf(like,d_2) = 1*0.3010\n",
      "$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "vectorizer = TfidfVectorizer()\n",
      "X_train = vectorizer.fit_transform(twenty_train_subset.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 1.07 s, sys: 74.9 ms, total: 1.14 s\n",
        "Wall time: 1.17 s\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can put this together with our other tricks as well...but notice the running time hit"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,5))\n",
      "X_train = vectorizer.fit_transform(twenty_train_subset.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 10.2 s, sys: 492 ms, total: 10.7 s\n",
        "Wall time: 10.8 s\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Random Forests"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[READ THE DOCS!](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use predict using our 20-newsgroup dataset above"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = TfidfVectorizer(stop_words='english')\n",
      "X_train = vectorizer.fit_transform(twenty_train_subset.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "tree_model = DecisionTreeClassifier()\n",
      "print cross_val_score(tree_model, X_train.toarray(), twenty_train_subset.target).mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.780202351178\n",
        "CPU times: user 20.9 s, sys: 1.38 s, total: 22.2 s\n",
        "Wall time: 23.3 s\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
<<<<<<< HEAD
      "rf_model = RandomForestClassifier(n_jobs = -1 ,n_estimators=1000)\n",
=======
      "rf_model = RandomForestClassifier(n_estimators=60)\n",
>>>>>>> 5ff02bdacae2a66fc9c21becf1d686e529da36c3
      "print cross_val_score(rf_model, X_train.toarray(), twenty_train_subset.target).mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.908549816559\n",
        "CPU times: user 9min 23s, sys: 3.75 s, total: 9min 27s\n",
        "Wall time: 2min 46s\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Getting Important Features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This prints the top 10 most important features\n",
      "rf_model.fit(X_train.toarray(),twenty_train_subset.target)\n",
      "sorted(zip(rf_model.feature_importances_, vectorizer.get_feature_names()), reverse=True)[:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "y_train = twenty_train_subset.target\n",
      "def plot_learning_curve(X_train, y_train, n_est=10):\n",
      "    for i in range(n_est):\n",
      "        errors = []\n",
      "        est = RandomForestClassifier(n_jobs = -1,n_estimators = i+1)\n",
      "        est.fit(X_train, y_train)\n",
      "        errors.append(cross_val_score(est, X_train, y_train).mean())        \n",
      "    fig, ax = plt.subplots()\n",
      "    ax.plot(i, errors, 'o-', color=\"r\", label='error')\n",
      "    ax.set_xlabel('n_estimators')\n",
      "    ax.set_ylabel('error')\n",
      "    ax.legend(loc=0)\n",
      "\n",
      "    \n",
      "    train_sizes, train_scores, test_scores = learning_curve(\n",
      "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
      "    train_scores_mean = np.mean(train_scores, axis=1)\n",
      "    train_scores_std = np.std(train_scores, axis=1)\n",
      "    test_scores_mean = np.mean(test_scores, axis=1)\n",
      "    test_scores_std = np.std(test_scores, axis=1)\n",
      "    \n",
      "    plt.grid()\n",
      "\n",
      "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
      "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
      "                     color=\"r\")\n",
      "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
      "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
      "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
      "             label=\"Training score\")\n",
      "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
      "             label=\"Cross-validation score\")\n",
      "\n",
      "    plt.legend(loc=\"best\")\n",
      "    return plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "y_train = twenty_train_subset.target\n",
      "def plot_learning_curve(X_train, y_train, n_est=10, n_jobs):\n",
      "    for i in range(n_est):\n",
      "        errors = []\n",
      "        est = RandomForestClassifier(n_jobs = -1,n_estimators = i+1)\n",
      "        est.fit(X_train, y_train)\n",
      "        errors.append(cross_val_score(est, X_train, y_train).mean())        \n",
      "        train_sizes, train_scores, test_scores = learning_curve(\n",
      "            estimator, X, y, n_jobs=n_jobs)\n",
      "        train_scores_mean = np.mean(train_scores, axis=1)\n",
      "        train_scores_std = np.std(train_scores, axis=1)\n",
      "        test_scores_mean = np.mean(test_scores, axis=1)\n",
      "        test_scores_std = np.std(test_scores, axis=1)\n",
      "    \n",
      "    plt.grid()\n",
      "\n",
      "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
      "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
      "                     color=\"r\")\n",
      "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
      "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
      "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
      "             label=\"Training score\")\n",
      "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
      "             label=\"Cross-validation score\")\n",
      "\n",
      "    plt.legend(loc=\"best\")\n",
      "    return plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_learning_curve(X_train.toarray(), twenty_train_subset.target, n_est = 15)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEQCAYAAABIqvhxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nM8vm5GFmZtmcPMzMLJuTh5mZZfv/cF22lv6BiY4AAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x100774490>"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.subplots().set_xlim?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Object `set_xlim` not found.\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.subplots().set_xlim"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Solution"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = TfidfVectorizer(stop_words='english')\n",
      "X_train = vectorizer.fit_transform(twenty_train_subset.data)\n",
      "#IMPORTANT, you should only transform you tested data, remember that in practice \n",
      "#you will not be able to retrain your model each time you get new data, so we \n",
      "#must simulate this constraint\n",
      "X_test = vectorizer.transform(twenty_test_subset.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.learning_curve import learning_curve\n",
      "\n",
      "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
      "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
      "    \"\"\"\n",
      "    Generate a simple plot of the test and traning learning curve.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
      "        An object of that type which is cloned for each validation.\n",
      "\n",
      "    title : string\n",
      "        Title for the chart.\n",
      "\n",
      "    X : array-like, shape (n_samples, n_features)\n",
      "        Training vector, where n_samples is the number of samples and\n",
      "        n_features is the number of features.\n",
      "\n",
      "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
      "        Target relative to X for classification or regression;\n",
      "        None for unsupervised learning.\n",
      "\n",
      "    ylim : tuple, shape (ymin, ymax), optional\n",
      "        Defines minimum and maximum yvalues plotted.\n",
      "\n",
      "    cv : integer, cross-validation generator, optional\n",
      "        If an integer is passed, it is the number of folds (defaults to 3).\n",
      "        Specific cross-validation objects can be passed, see\n",
      "        sklearn.cross_validation module for the list of possible objects\n",
      "\n",
      "    n_jobs : integer, optional\n",
      "        Number of jobs to run in parallel (default 1).\n",
      "    \"\"\"\n",
      "    plt.figure()\n",
      "    plt.title(title)\n",
      "    if ylim is not None:\n",
      "        plt.ylim(*ylim)\n",
      "    plt.xlabel(\"Training examples\")\n",
      "    plt.ylabel(\"Score\")\n",
      "    train_sizes, train_scores, test_scores = learning_curve(\n",
      "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
      "    train_scores_mean = np.mean(train_scores, axis=1)\n",
      "    train_scores_std = np.std(train_scores, axis=1)\n",
      "    test_scores_mean = np.mean(test_scores, axis=1)\n",
      "    test_scores_std = np.std(test_scores, axis=1)\n",
      "    plt.grid()\n",
      "\n",
      "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
      "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
      "                     color=\"r\")\n",
      "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
      "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
      "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
      "             label=\"Training score\")\n",
      "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
      "             label=\"Cross-validation score\")\n",
      "\n",
      "    plt.legend(loc=\"best\")\n",
      "    return plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "#this is an example of 1 learning curve for 1 model, try a few more.\n",
      "_ = plot_learning_curve(RandomForestClassifier(n_estimators=100),'test',X_train.toarray(),twenty_train_subset.target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rf_model = RandomForestClassifier(n_estimators=100,max_depth=15,criterion='entropy')\n",
      "rf_model.fit(X_train.toarray(),twenty_train_subset.target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_pred = rf_model.predict(X_test.toarray())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import confusion_matrix\n",
      "\n",
      "def plot_confusion_matrix(y_pred, y):\n",
      "    plt.imshow(confusion_matrix(y, y_pred),\n",
      "               cmap=plt.cm.binary, interpolation='nearest')\n",
      "    plt.colorbar()\n",
      "    plt.xlabel('true value')\n",
      "    plt.ylabel('predicted value')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_confusion_matrix( y_pred,twenty_test_subset.target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import classification_report\n",
      "print classification_report(twenty_test_subset.target,y_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}