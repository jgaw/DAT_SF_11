{
 "metadata": {
  "name": "",
  "signature": "sha256:f8b8a279f6ba5b83b60ccd9a25ec1fa5ea2025147357e04dfe1908f2e8ededc9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Using tf-idf and random forests"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Newsgroups Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will take a look at some of the twenty newsgroups dataset, another common dataset for classification. Note that the data is fetched from."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import fetch_20newsgroups\n",
      "\n",
      "# We will use four of the twenty newsgroups\n",
      "categories = ['alt.atheism',\n",
      "              'talk.religion.misc',\n",
      "              'comp.graphics',\n",
      "              'sci.space']\n",
      "\n",
      "twenty_train_subset = fetch_20newsgroups(subset='train', categories=categories)\n",
      "twenty_test_subset = fetch_20newsgroups(subset='test', categories=categories)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:sklearn.datasets.twenty_newsgroups:Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we have lists of messages (as strings) in the `.data` members."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "datasets.get_data_home()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "'/Users/jeremiah/scikit_learn_data'"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Features from text"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here are some ways to generate features from the text:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Count Vectorizer\n",
      "Count Vectorizer is the easiest text processing utility to understand, it simply counts the occurances of non-stopwords. (what is a stopword?)<br>\n",
      "First, let's check out our data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print twenty_train_subset.data[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "From: rych@festival.ed.ac.uk (R Hawkes)\n",
        "Subject: 3DS: Where did all the texture rules go?\n",
        "Lines: 21\n",
        "\n",
        "Hi,\n",
        "\n",
        "I've noticed that if you only save a model (with all your mapping planes\n",
        "positioned carefully) to a .3DS file that when you reload it after restarting\n",
        "3DS, they are given a default position and orientation.  But if you save\n",
        "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
        "know why this information is not stored in the .3DS file?  Nothing is\n",
        "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
        "I'd like to be able to read the texture rule information, does anyone have \n",
        "the format for the .PRJ file?\n",
        "\n",
        "Is the .CEL file format available from somewhere?\n",
        "\n",
        "Rych\n",
        "\n",
        "======================================================================\n",
        "Rycharde Hawkes\t\t\t\temail: rych@festival.ed.ac.uk\n",
        "Virtual Environment Laboratory\n",
        "Dept. of Psychology\t\t\tTel  : +44 31 650 3426\n",
        "Univ. of Edinburgh\t\t\tFax  : +44 31 667 0150\n",
        "======================================================================\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "vectorizer = CountVectorizer(stop_words='english')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "CountVectorizer(analyzer=u'word', binary=False, charset=None,\n",
        "        charset_error=None, decode_error=u'strict',\n",
        "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
        "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
        "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
        "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
        "        tokenizer=None, vocabulary=None)"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "one_message = vectorizer.fit_transform(twenty_train_subset.data[0:3]).todense().tolist()[0]\n",
      "zip(vectorizer.get_feature_names(),one_message)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "[(u'0150', 1L),\n",
        " (u'02', 0L),\n",
        " (u'020359', 0L),\n",
        " (u'04', 0L),\n",
        " (u'07', 0L),\n",
        " (u'08', 0L),\n",
        " (u'15', 0L),\n",
        " (u'19', 0L),\n",
        " (u'1970', 0L),\n",
        " (u'1993', 0L),\n",
        " (u'1993apr19', 0L),\n",
        " (u'20', 0L),\n",
        " (u'20apr199301460499', 0L),\n",
        " (u'21', 1L),\n",
        " (u'23', 0L),\n",
        " (u'2400x4', 0L),\n",
        " (u'245', 0L),\n",
        " (u'26996', 0L),\n",
        " (u'31', 2L),\n",
        " (u'3205', 0L),\n",
        " (u'3426', 1L),\n",
        " (u'3ds', 4L),\n",
        " (u'4366', 0L),\n",
        " (u'44', 2L),\n",
        " (u'55', 0L),\n",
        " (u'604', 0L),\n",
        " (u'650', 1L),\n",
        " (u'667', 1L),\n",
        " (u'_perijoves_', 0L),\n",
        " (u'able', 1L),\n",
        " (u'ac', 2L),\n",
        " (u'access', 0L),\n",
        " (u'actually', 0L),\n",
        " (u'almanac', 0L),\n",
        " (u'apoapsis', 0L),\n",
        " (u'apr', 0L),\n",
        " (u'article', 0L),\n",
        " (u'available', 1L),\n",
        " (u'b645zaw', 0L),\n",
        " (u'backing', 0L),\n",
        " (u'barring', 0L),\n",
        " (u'bc', 0L),\n",
        " (u'biblical', 0L),\n",
        " (u'brader', 0L),\n",
        " (u'british', 0L),\n",
        " (u'bunch', 0L),\n",
        " (u'ca', 0L),\n",
        " (u'canada', 0L),\n",
        " (u'carefully', 1L),\n",
        " (u'carried', 0L),\n",
        " (u'cel', 1L),\n",
        " (u'central', 0L),\n",
        " (u'centuries', 0L),\n",
        " (u'children', 0L),\n",
        " (u'circa', 0L),\n",
        " (u'cites', 0L),\n",
        " (u'columbia', 0L),\n",
        " (u'com', 0L),\n",
        " (u'comet', 0L),\n",
        " (u'contrary', 0L),\n",
        " (u'corruption', 0L),\n",
        " (u'couldn', 0L),\n",
        " (u'default', 1L),\n",
        " (u'delusional', 0L),\n",
        " (u'demonstrating', 0L),\n",
        " (u'dept', 1L),\n",
        " (u'deranged', 0L),\n",
        " (u'did', 1L),\n",
        " (u'does', 2L),\n",
        " (u'ed', 2L),\n",
        " (u'edinburgh', 1L),\n",
        " (u'edu', 0L),\n",
        " (u'email', 1L),\n",
        " (u'enclosed', 0L),\n",
        " (u'environment', 1L),\n",
        " (u'evidence', 0L),\n",
        " (u'evil', 0L),\n",
        " (u'explicitly', 1L),\n",
        " (u'f208', 0L),\n",
        " (u'fanatic', 0L),\n",
        " (u'fax', 1L),\n",
        " (u'feb', 0L),\n",
        " (u'festival', 2L),\n",
        " (u'fidonet', 0L),\n",
        " (u'figure', 0L),\n",
        " (u'file', 6L),\n",
        " (u'fisher', 0L),\n",
        " (u'folks', 0L),\n",
        " (u'format', 2L),\n",
        " (u'frog', 0L),\n",
        " (u'fruitcakes', 0L),\n",
        " (u'given', 1L),\n",
        " (u'got', 0L),\n",
        " (u'gotten', 0L),\n",
        " (u'gt', 0L),\n",
        " (u'hawkes', 2L),\n",
        " (u'hi', 1L),\n",
        " (u'hisse', 0L),\n",
        " (u'holocaust', 0L),\n",
        " (u'home', 0L),\n",
        " (u'information', 2L),\n",
        " (u'internet', 0L),\n",
        " (u'island', 0L),\n",
        " (u'jg', 0L),\n",
        " (u'jgarland', 0L),\n",
        " (u'jim', 0L),\n",
        " (u'jones', 0L),\n",
        " (u'jupiter', 0L),\n",
        " (u'just', 0L),\n",
        " (u'kean', 0L),\n",
        " (u'ken', 0L),\n",
        " (u'killed', 0L),\n",
        " (u'kmcvay', 0L),\n",
        " (u'know', 1L),\n",
        " (u'koresh', 0L),\n",
        " (u'laboratory', 1L),\n",
        " (u'ladysmith', 0L),\n",
        " (u'language', 0L),\n",
        " (u'learned', 0L),\n",
        " (u'like', 1L),\n",
        " (u'lines', 1L),\n",
        " (u'lot', 0L),\n",
        " (u'mail', 0L),\n",
        " (u'mania', 0L),\n",
        " (u'manual', 1L),\n",
        " (u'mapping', 1L),\n",
        " (u'mark', 0L),\n",
        " (u'mb', 0L),\n",
        " (u'mcvay', 0L),\n",
        " (u'mean', 0L),\n",
        " (u'message', 0L),\n",
        " (u'messenger', 0L),\n",
        " (u'model', 1L),\n",
        " (u'msb', 0L),\n",
        " (u'msged', 0L),\n",
        " (u'mun', 0L),\n",
        " (u'n103', 0L),\n",
        " (u'neccessary', 0L),\n",
        " (u'newtout', 0L),\n",
        " (u'nope', 0L),\n",
        " (u'noticed', 1L),\n",
        " (u'old', 0L),\n",
        " (u'oneb', 0L),\n",
        " (u'orbit', 0L),\n",
        " (u'org', 0L),\n",
        " (u'organization', 0L),\n",
        " (u'orientation', 2L),\n",
        " (u'p201', 0L),\n",
        " (u'perew', 0L),\n",
        " (u'periapsis', 0L),\n",
        " (u'perijove', 0L),\n",
        " (u'planes', 1L),\n",
        " (u'position', 1L),\n",
        " (u'positioned', 1L),\n",
        " (u'positions', 1L),\n",
        " (u'preserved', 1L),\n",
        " (u'prj', 3L),\n",
        " (u'psychology', 1L),\n",
        " (u'public', 0L),\n",
        " (u'read', 1L),\n",
        " (u'reload', 1L),\n",
        " (u'restarting', 1L),\n",
        " (u'rotten', 0L),\n",
        " (u'rule', 1L),\n",
        " (u'rules', 2L),\n",
        " (u'rych', 3L),\n",
        " (u'rycharde', 1L),\n",
        " (u'ryugen', 0L),\n",
        " (u'said', 1L),\n",
        " (u'salute', 0L),\n",
        " (u'satisfy', 0L),\n",
        " (u'save', 2L),\n",
        " (u'saving', 1L),\n",
        " (u'say', 0L),\n",
        " (u'says', 0L),\n",
        " (u'sco', 0L),\n",
        " (u'sender', 0L),\n",
        " (u'serving', 0L),\n",
        " (u'simply', 0L),\n",
        " (u'sorry', 0L),\n",
        " (u'sq', 0L),\n",
        " (u'stephen', 0L),\n",
        " (u'stored', 1L),\n",
        " (u'subject', 1L),\n",
        " (u'sure', 0L),\n",
        " (u'surprised', 0L),\n",
        " (u'talking', 0L),\n",
        " (u'tape', 0L),\n",
        " (u'tel', 1L),\n",
        " (u'temporary', 0L),\n",
        " (u'texture', 3L),\n",
        " (u'things', 0L),\n",
        " (u'thought', 0L),\n",
        " (u'time', 0L),\n",
        " (u'ucs', 0L),\n",
        " (u'uk', 2L),\n",
        " (u'univ', 1L),\n",
        " (u'unlikely', 0L),\n",
        " (u'used', 0L),\n",
        " (u'usenet', 0L),\n",
        " (u'uta', 0L),\n",
        " (u'utarlg', 0L),\n",
        " (u'v32', 0L),\n",
        " (u'vancouver', 0L),\n",
        " (u've', 1L),\n",
        " (u'virtual', 1L),\n",
        " (u'writes', 0L),\n",
        " (u'xenix', 0L),\n",
        " (u'z1', 0L)]"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This makes a matrix of word counts, where each row is a document and each column is the word, the cell matrix[document, word] contains the count of word in document.\n",
      "<br><br>\n",
      "Now try this with the whole training subset:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train = vectorizer.fit_transform(twenty_train_subset.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "<2034x33815 sparse matrix of type '<type 'numpy.int64'>'\n",
        "\twith 233470 stored elements in Compressed Sparse Row format>"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By default this returns a sparse matrix, which will save memory.\n",
      "\n",
      "Also, notice our stop words:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer.get_stop_words()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "frozenset({'a',\n",
        "           'about',\n",
        "           'above',\n",
        "           'across',\n",
        "           'after',\n",
        "           'afterwards',\n",
        "           'again',\n",
        "           'against',\n",
        "           'all',\n",
        "           'almost',\n",
        "           'alone',\n",
        "           'along',\n",
        "           'already',\n",
        "           'also',\n",
        "           'although',\n",
        "           'always',\n",
        "           'am',\n",
        "           'among',\n",
        "           'amongst',\n",
        "           'amoungst',\n",
        "           'amount',\n",
        "           'an',\n",
        "           'and',\n",
        "           'another',\n",
        "           'any',\n",
        "           'anyhow',\n",
        "           'anyone',\n",
        "           'anything',\n",
        "           'anyway',\n",
        "           'anywhere',\n",
        "           'are',\n",
        "           'around',\n",
        "           'as',\n",
        "           'at',\n",
        "           'back',\n",
        "           'be',\n",
        "           'became',\n",
        "           'because',\n",
        "           'become',\n",
        "           'becomes',\n",
        "           'becoming',\n",
        "           'been',\n",
        "           'before',\n",
        "           'beforehand',\n",
        "           'behind',\n",
        "           'being',\n",
        "           'below',\n",
        "           'beside',\n",
        "           'besides',\n",
        "           'between',\n",
        "           'beyond',\n",
        "           'bill',\n",
        "           'both',\n",
        "           'bottom',\n",
        "           'but',\n",
        "           'by',\n",
        "           'call',\n",
        "           'can',\n",
        "           'cannot',\n",
        "           'cant',\n",
        "           'co',\n",
        "           'con',\n",
        "           'could',\n",
        "           'couldnt',\n",
        "           'cry',\n",
        "           'de',\n",
        "           'describe',\n",
        "           'detail',\n",
        "           'do',\n",
        "           'done',\n",
        "           'down',\n",
        "           'due',\n",
        "           'during',\n",
        "           'each',\n",
        "           'eg',\n",
        "           'eight',\n",
        "           'either',\n",
        "           'eleven',\n",
        "           'else',\n",
        "           'elsewhere',\n",
        "           'empty',\n",
        "           'enough',\n",
        "           'etc',\n",
        "           'even',\n",
        "           'ever',\n",
        "           'every',\n",
        "           'everyone',\n",
        "           'everything',\n",
        "           'everywhere',\n",
        "           'except',\n",
        "           'few',\n",
        "           'fifteen',\n",
        "           'fify',\n",
        "           'fill',\n",
        "           'find',\n",
        "           'fire',\n",
        "           'first',\n",
        "           'five',\n",
        "           'for',\n",
        "           'former',\n",
        "           'formerly',\n",
        "           'forty',\n",
        "           'found',\n",
        "           'four',\n",
        "           'from',\n",
        "           'front',\n",
        "           'full',\n",
        "           'further',\n",
        "           'get',\n",
        "           'give',\n",
        "           'go',\n",
        "           'had',\n",
        "           'has',\n",
        "           'hasnt',\n",
        "           'have',\n",
        "           'he',\n",
        "           'hence',\n",
        "           'her',\n",
        "           'here',\n",
        "           'hereafter',\n",
        "           'hereby',\n",
        "           'herein',\n",
        "           'hereupon',\n",
        "           'hers',\n",
        "           'herself',\n",
        "           'him',\n",
        "           'himself',\n",
        "           'his',\n",
        "           'how',\n",
        "           'however',\n",
        "           'hundred',\n",
        "           'i',\n",
        "           'ie',\n",
        "           'if',\n",
        "           'in',\n",
        "           'inc',\n",
        "           'indeed',\n",
        "           'interest',\n",
        "           'into',\n",
        "           'is',\n",
        "           'it',\n",
        "           'its',\n",
        "           'itself',\n",
        "           'keep',\n",
        "           'last',\n",
        "           'latter',\n",
        "           'latterly',\n",
        "           'least',\n",
        "           'less',\n",
        "           'ltd',\n",
        "           'made',\n",
        "           'many',\n",
        "           'may',\n",
        "           'me',\n",
        "           'meanwhile',\n",
        "           'might',\n",
        "           'mill',\n",
        "           'mine',\n",
        "           'more',\n",
        "           'moreover',\n",
        "           'most',\n",
        "           'mostly',\n",
        "           'move',\n",
        "           'much',\n",
        "           'must',\n",
        "           'my',\n",
        "           'myself',\n",
        "           'name',\n",
        "           'namely',\n",
        "           'neither',\n",
        "           'never',\n",
        "           'nevertheless',\n",
        "           'next',\n",
        "           'nine',\n",
        "           'no',\n",
        "           'nobody',\n",
        "           'none',\n",
        "           'noone',\n",
        "           'nor',\n",
        "           'not',\n",
        "           'nothing',\n",
        "           'now',\n",
        "           'nowhere',\n",
        "           'of',\n",
        "           'off',\n",
        "           'often',\n",
        "           'on',\n",
        "           'once',\n",
        "           'one',\n",
        "           'only',\n",
        "           'onto',\n",
        "           'or',\n",
        "           'other',\n",
        "           'others',\n",
        "           'otherwise',\n",
        "           'our',\n",
        "           'ours',\n",
        "           'ourselves',\n",
        "           'out',\n",
        "           'over',\n",
        "           'own',\n",
        "           'part',\n",
        "           'per',\n",
        "           'perhaps',\n",
        "           'please',\n",
        "           'put',\n",
        "           'rather',\n",
        "           're',\n",
        "           'same',\n",
        "           'see',\n",
        "           'seem',\n",
        "           'seemed',\n",
        "           'seeming',\n",
        "           'seems',\n",
        "           'serious',\n",
        "           'several',\n",
        "           'she',\n",
        "           'should',\n",
        "           'show',\n",
        "           'side',\n",
        "           'since',\n",
        "           'sincere',\n",
        "           'six',\n",
        "           'sixty',\n",
        "           'so',\n",
        "           'some',\n",
        "           'somehow',\n",
        "           'someone',\n",
        "           'something',\n",
        "           'sometime',\n",
        "           'sometimes',\n",
        "           'somewhere',\n",
        "           'still',\n",
        "           'such',\n",
        "           'system',\n",
        "           'take',\n",
        "           'ten',\n",
        "           'than',\n",
        "           'that',\n",
        "           'the',\n",
        "           'their',\n",
        "           'them',\n",
        "           'themselves',\n",
        "           'then',\n",
        "           'thence',\n",
        "           'there',\n",
        "           'thereafter',\n",
        "           'thereby',\n",
        "           'therefore',\n",
        "           'therein',\n",
        "           'thereupon',\n",
        "           'these',\n",
        "           'they',\n",
        "           'thick',\n",
        "           'thin',\n",
        "           'third',\n",
        "           'this',\n",
        "           'those',\n",
        "           'though',\n",
        "           'three',\n",
        "           'through',\n",
        "           'throughout',\n",
        "           'thru',\n",
        "           'thus',\n",
        "           'to',\n",
        "           'together',\n",
        "           'too',\n",
        "           'top',\n",
        "           'toward',\n",
        "           'towards',\n",
        "           'twelve',\n",
        "           'twenty',\n",
        "           'two',\n",
        "           'un',\n",
        "           'under',\n",
        "           'until',\n",
        "           'up',\n",
        "           'upon',\n",
        "           'us',\n",
        "           'very',\n",
        "           'via',\n",
        "           'was',\n",
        "           'we',\n",
        "           'well',\n",
        "           'were',\n",
        "           'what',\n",
        "           'whatever',\n",
        "           'when',\n",
        "           'whence',\n",
        "           'whenever',\n",
        "           'where',\n",
        "           'whereafter',\n",
        "           'whereas',\n",
        "           'whereby',\n",
        "           'wherein',\n",
        "           'whereupon',\n",
        "           'wherever',\n",
        "           'whether',\n",
        "           'which',\n",
        "           'while',\n",
        "           'whither',\n",
        "           'who',\n",
        "           'whoever',\n",
        "           'whole',\n",
        "           'whom',\n",
        "           'whose',\n",
        "           'why',\n",
        "           'will',\n",
        "           'with',\n",
        "           'within',\n",
        "           'without',\n",
        "           'would',\n",
        "           'yet',\n",
        "           'you',\n",
        "           'your',\n",
        "           'yours',\n",
        "           'yourself',\n",
        "           'yourselves'})"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###n-gram\n",
      "The basic idea of n-gram is to take a sequence of objects and make sense of it. Take, for example:<br><br>\n",
      "\n",
      "`I am Sam,`<br>\n",
      "`Sam I am,`<br>\n",
      "`Do you like green eggs and ham?`<br><br>\n",
      "\n",
      "To gram this we will extract all sequences of length `n` like so (for `n=3`):<br><br>\n",
      "`(i,am,sam),(am,sam,sam),(sam,sam,i),(sam,i,am),...`<br><br>\n",
      "\n",
      "scikit gives us the option to use n-grams as features their extraction module:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Include every 1-gram, 2-gram, and 3-gram\n",
      "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
      "X_train = vectorizer.fit_transform(twenty_train_subset.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that this heavily inflates feature set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "<2034x688611 sparse matrix of type '<type 'numpy.int64'>'\n",
        "\twith 1437560 stored elements in Compressed Sparse Row format>"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###TF-IDF"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Additionally, we could use a tf-idf representation, which stands for Term Frequency, Inverse Document Frequency.\n",
      "\n",
      "This value is the product of two intermediate values, the Term Frequency and the Inverse Document Frequency.\n",
      "\n",
      "The Term Frequency is equivalent to the `CountVectorizer` features, the number of times or count that a word appear in the document. This is our most basic representation of text.\n",
      "\n",
      "To establish Inverse Document Frequency, first let's define Document Frequency. This is the percentage of documents that a particular word appears in. For example, the word `the` might appear in 100% of documents, while words like `Syria` would likely have low document frequency. Inverse Document Frequency is simply 1 / Document Frequency (although often the log is also taken).\n",
      "\n",
      "Let, $D$ be the set of all documents:\n",
      "$$\n",
      "idf(word,D) = \\log \\frac{N}{|\\{d \\in D : t \\in d\\}|}\n",
      "$$\n",
      "\n",
      "Notice that this has the neat property that if a word occurs in ALL documents, $idf = 0$ so this naturally controls for stop words \n",
      "\n",
      "So tf-idf is Term Frequency * Inverse Document Frequency, or similar to Term Frequency / Document Frequency. The intuition is that words that have high weight are those that appear a lot in this document and/or appear in very few other documents (somehow unique to this document).\n",
      "\n",
      "Example:\n",
      "Let, $d_1 = $\"i am sam sam i am\", and $d_2 = $ \"i do not like them sam i am\", then:\n",
      "$$\n",
      "tf(like,d_2) = 1\n",
      "$$\n",
      "$$\n",
      "idf(like,D) = \\log \\frac{2}{1} = 0.3010\n",
      "$$\n",
      "$$\n",
      "tfidf(like,d_2) = 1*0.3010\n",
      "$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "vectorizer = TfidfVectorizer()\n",
      "X_train = vectorizer.fit_transform(twenty_train_subset.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 1.07 s, sys: 74.9 ms, total: 1.14 s\n",
        "Wall time: 1.17 s\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can put this together with our other tricks as well...but notice the running time hit"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,5))\n",
      "X_train = vectorizer.fit_transform(twenty_train_subset.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 10.2 s, sys: 492 ms, total: 10.7 s\n",
        "Wall time: 10.8 s\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Random Forests"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[READ THE DOCS!](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use predict using our 20-newsgroup dataset above"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = TfidfVectorizer(stop_words='english')\n",
      "X_train = vectorizer.fit_transform(twenty_train_subset.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "tree_model = DecisionTreeClassifier()\n",
      "print cross_val_score(tree_model, X_train.toarray(), twenty_train_subset.target).mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.780202351178\n",
        "CPU times: user 20.9 s, sys: 1.38 s, total: 22.2 s\n",
        "Wall time: 23.3 s\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "rf_model = RandomForestClassifier(n_jobs = -1 ,n_estimators=1000)\n",
      "print cross_val_score(rf_model, X_train.toarray(), twenty_train_subset.target).mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.908549816559\n",
        "CPU times: user 9min 23s, sys: 3.75 s, total: 9min 27s\n",
        "Wall time: 2min 46s\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Getting Important Features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This prints the top 10 most important features\n",
      "rf_model.fit(X_train.toarray(),twenty_train_subset.target)\n",
      "sorted(zip(rf_model.feature_importances_, vectorizer.get_feature_names()), reverse=True)[:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "from sklearn.metrics import log_loss\n",
      "\n",
      "def compute_error(x, y, p):\n",
      "    yfit = np.polyval(p, x)\n",
      "    return np.sqrt(np.mean((y - yfit) ** 2))\n",
      "\n",
      "\n",
      "def plot_learning_curve(X_train, y_train, n_est=5):\n",
      "    errors_train = []\n",
      "    errors_test = []\n",
      "    for i in range(n_est):\n",
      "        est = RandomForestClassifier(n_jobs = -1,n_estimators = i+1)\n",
      "        est.fit(X_train, y_train)\n",
      "        #errors_train.append(cross_val_score(est,X_train, y_train).mean()) \n",
      "        errors_test.append(mean_squared_error(est.predict(X_train),y_train))\n",
      "    fig, ax = plt.subplots()\n",
      "    #ax.plot(range(n_est), errors_train, 'o-', color=\"g\", label='error_train')\n",
      "    ax.plot(range(n_est), errors_test, 'o-', color=\"r\", label='mse')\n",
      "    ax.set_xlabel('n_estimators')\n",
      "    ax.set_ylabel('error')\n",
      "    ax.legend(loc=0)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 106
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_train = twenty_train_subset.target\n",
      "\n",
      "errors_test.append(mean_squared_error(est.predict(X_train),y_train))\n",
      "\n",
      "print np.shape(y_train)\n",
      "print np.shape(X_train.toarray())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(2034,)\n",
        "(2034, 33815)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 101
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_learning_curve(X_train.toarray(), y_train, n_est= 20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEQCAYAAACwSgOGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndH4AAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1100d3c90>"
       ]
      }
     ],
     "prompt_number": 107
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#interpreted as the probability that the classifier will assign a higher score to a randomly chosen positive example \n",
      "#than to a randomly chosen negative example \n",
      "\n",
      "#leonid.e.zhukov@gmail.com\n",
      "#get ramesh's email"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}